{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104f618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7bcac",
   "metadata": {},
   "source": [
    "#### Documents collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13885487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 pages from PDFs\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for pdf_path in glob.glob(\"documents/*.pdf\"):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages from PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83da7f4",
   "metadata": {},
   "source": [
    "#### Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b47191d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 10 documents into 34 chunks\n",
      "\n",
      "Chunk 1: Personal Biography: Victor Ridwan Ademuyiwa \n",
      "Introduction \n",
      "Victor Ridwan Ademuyiwa is a driven and purpose-oriented individual whose journey \n",
      "reflects resilience, curiosity, and a deep commitment to growth. Born on 27th May and raised \n",
      "in Ijoko, Sango Ota, Ogun State, he has steadily built a path defined by learning, discipline, \n",
      "and a passion for technology. \n",
      "Early Life \n",
      "Growing up in Ijoko shaped Ridwan‚Äôs values and worldview. His environment taught him\n",
      "\n",
      "Chunk 2: Early Life \n",
      "Growing up in Ijoko shaped Ridwan‚Äôs values and worldview. His environment taught him \n",
      "patience, determination, and the belief that progress is not a race but a steady climb. These \n",
      "early experiences became the foundation of his personal philosophy: ‚ÄúSlow and steady ‚Äî it \n",
      "is not about how far, but how well.‚Äù \n",
      "Education \n",
      "Ridwan‚Äôs academic journey reflects both breadth and depth: \n",
      "‚Ä¢ B.Sc. Industrial Chemistry \n",
      "Federal University of Petroleum Resources, Effurun (2017‚Äì2023)\n"
     ]
    }
   ],
   "source": [
    "# Create splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf97faa",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d24ed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "print(\"Embeddings created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48504251",
   "metadata": {},
   "source": [
    "#### Vector Store\n",
    "# Create vector store from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8151c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chroma_db loaded\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"./chroma_db\"\n",
    "collection_name = \"collection\"\n",
    "\n",
    "if os.path.exists(persist_directory):\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"collection\",\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    print(\"chroma_db loaded\")\n",
    "else:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        collection_name=\"collection\",\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "    print(\"chroma_db created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062785b",
   "metadata": {},
   "source": [
    "#### Conversational RAG with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12079bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Store for chat histories\n",
    "chat_store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        chat_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "# Create conversational prompt\n",
    "conv_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are the personal AI assistant for Victor Ridwan Ademuyiwa. \"\n",
    "     \"You must answer questions ONLY if:\\n\"\n",
    "     \"1. The question is clear, complete, and meaningful.\\n\"\n",
    "     \"2. The provided context directly contains the information needed.\\n\\n\"\n",
    "     \"If the question is unclear, incomplete, or nonsensical, respond exactly with:\\n\"\n",
    "     \"'Please ask a clear and complete question.'\\n\\n\"\n",
    "     \"If the context does not contain the answer, respond exactly with:\\n\"\n",
    "     \"'I don‚Äôt have information about this in the provided documents.'\\n\\n\"\n",
    "     \"Do NOT guess, infer, or generalize beyond the context.\"\n",
    "    ),\n",
    "\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "\n",
    "    (\"human\",\n",
    "     \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"),\n",
    "\n",
    "    (\"system\",\n",
    "     \"Rules for formatting the final answer:\\n\"\n",
    "     \"- Answer in clear, concise sentences.\\n\"\n",
    "     \"- List sources as bullet points ONLY if you actually used the context.\\n\"\n",
    "     \"- If you did not use the context, do NOT list any sources.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('source', 'unknown')}\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "# Build base chain\n",
    "conv_chain_base = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        question=lambda x: x[\"question\"],\n",
    "        chat_history=lambda x: x.get(\"chat_history\", [])\n",
    "    )\n",
    "    | conv_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with message history\n",
    "conv_chain = RunnableWithMessageHistory(\n",
    "    conv_chain_base,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdecec4",
   "metadata": {},
   "source": [
    "\n",
    "**Example interactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec58bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      " Victor Ridwan Ademuyiwa has worked on projects ranging from AI systems to interactive web apps, showcasing his ability to learn, build, and solve problems. His projects demonstrate consistent growth, curiosity, and a willingness to explore new technologies.\n",
      "\n",
      "Response 2:\n",
      " The AI and Machine Learning projects that Victor Ridwan Ademuyiwa has worked on include:\n",
      "- RAG Chatbot (Personal AI Assistant)\n",
      "- Student Performance Prediction\n",
      "- Car Price Prediction System\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "response = conv_chain.invoke(\n",
    "    {\"question\": \"what projects has Victor worked on\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(\"Response 1:\\n\", response)\n",
    "\n",
    "# Follow-up question\n",
    "response2 = conv_chain.invoke(\n",
    "    {\"question\": \"Which of those projects are AI and Machine Learning project?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "\n",
    "print(\"\\nResponse 2:\\n\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7a60a",
   "metadata": {},
   "source": [
    "#### Users Interactive Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23044138",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"user_1\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nAsk your question about Victor (or type 'exit' to quit): \")\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(Markdown(\"### Goodbye! Have a nice day.\"))\n",
    "        break\n",
    "\n",
    "    response = conv_chain.invoke(\n",
    "        {\"question\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # print(\"\\nResponce:\", response)\n",
    "    display(Markdown(f\"\"\"\n",
    "---\n",
    "**üßë User:**  \n",
    "{user_input}\n",
    "\n",
    "**ü§ñ Response:**  \n",
    "{response}\n",
    "\"\"\"))\n",
    "    \n",
    "clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e11bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
